{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1d118e0",
   "metadata": {},
   "source": [
    "# Fake news article Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9beb3f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('always')\n",
    "\n",
    "fake = pd.read_csv(\"./dataset/Fake.csv\")\n",
    "true = pd.read_csv(\"./dataset/True.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ccbc3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fake shape:  (23481, 4)\n",
      "True shape:  (21417, 4)\n"
     ]
    }
   ],
   "source": [
    "print(\"Fake shape: \", fake.shape)\n",
    "print(\"True shape: \", true.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "446e194c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
       "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 30, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump Is So Obsessed He Even Has Obama’s Name...</td>\n",
       "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 29, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
       "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 25, 2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0   Donald Trump Sends Out Embarrassing New Year’...   \n",
       "1   Drunk Bragging Trump Staffer Started Russian ...   \n",
       "2   Sheriff David Clarke Becomes An Internet Joke...   \n",
       "3   Trump Is So Obsessed He Even Has Obama’s Name...   \n",
       "4   Pope Francis Just Called Out Donald Trump Dur...   \n",
       "\n",
       "                                                text subject  \\\n",
       "0  Donald Trump just couldn t wish all Americans ...    News   \n",
       "1  House Intelligence Committee Chairman Devin Nu...    News   \n",
       "2  On Friday, it was revealed that former Milwauk...    News   \n",
       "3  On Christmas day, Donald Trump announced that ...    News   \n",
       "4  Pope Francis used his annual Christmas Day mes...    News   \n",
       "\n",
       "                date  \n",
       "0  December 31, 2017  \n",
       "1  December 31, 2017  \n",
       "2  December 30, 2017  \n",
       "3  December 29, 2017  \n",
       "4  December 25, 2017  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abb2db92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>As U.S. budget fight looms, Republicans flip t...</td>\n",
       "      <td>WASHINGTON (Reuters) - The head of a conservat...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U.S. military to accept transgender recruits o...</td>\n",
       "      <td>WASHINGTON (Reuters) - Transgender people will...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 29, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior U.S. Republican senator: 'Let Mr. Muell...</td>\n",
       "      <td>WASHINGTON (Reuters) - The special counsel inv...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FBI Russia probe helped by Australian diplomat...</td>\n",
       "      <td>WASHINGTON (Reuters) - Trump campaign adviser ...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 30, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trump wants Postal Service to charge 'much mor...</td>\n",
       "      <td>SEATTLE/WASHINGTON (Reuters) - President Donal...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 29, 2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  As U.S. budget fight looms, Republicans flip t...   \n",
       "1  U.S. military to accept transgender recruits o...   \n",
       "2  Senior U.S. Republican senator: 'Let Mr. Muell...   \n",
       "3  FBI Russia probe helped by Australian diplomat...   \n",
       "4  Trump wants Postal Service to charge 'much mor...   \n",
       "\n",
       "                                                text       subject  \\\n",
       "0  WASHINGTON (Reuters) - The head of a conservat...  politicsNews   \n",
       "1  WASHINGTON (Reuters) - Transgender people will...  politicsNews   \n",
       "2  WASHINGTON (Reuters) - The special counsel inv...  politicsNews   \n",
       "3  WASHINGTON (Reuters) - Trump campaign adviser ...  politicsNews   \n",
       "4  SEATTLE/WASHINGTON (Reuters) - President Donal...  politicsNews   \n",
       "\n",
       "                 date  \n",
       "0  December 31, 2017   \n",
       "1  December 29, 2017   \n",
       "2  December 31, 2017   \n",
       "3  December 30, 2017   \n",
       "4  December 29, 2017   "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3ea325",
   "metadata": {},
   "source": [
    "### Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed36075f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake[\"label\"] = 0  # 0 = Fake\n",
    "true[\"label\"] = 1  # 1 = Real\n",
    "\n",
    "df = pd.concat([fake, true], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0905de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title      0\n",
      "text       0\n",
      "subject    0\n",
      "date       0\n",
      "label      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bc9b5d",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bfefac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e0923b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6898462a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uppercase to lowercase\n",
    "df[\"text\"] = df[\"text\"].str.lower()\n",
    "\n",
    "# deleting link and stuff XD\n",
    "df[\"text\"] = df[\"text\"].apply(lambda x: re.sub(r'http\\S+|www\\S+|https\\S+', '', x))  \n",
    "df[\"text\"] = df[\"text\"].apply(lambda x: re.sub(r'\\d+', '', x))  \n",
    "df[\"text\"] = df[\"text\"].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40bed27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df[\"text\"] = df[\"text\"].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe4f5034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df[\"text\"] = df[\"text\"].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9057e2a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    donald trump wish american happy new year leav...\n",
       "1    house intelligence committee chairman devin nu...\n",
       "2    friday revealed former milwaukee sheriff david...\n",
       "3    christmas day donald trump announced would bac...\n",
       "4    pope francis used annual christmas day message...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"text\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41dd0ed",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebbe5e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      4696\n",
      "           1       0.99      0.99      0.99      4284\n",
      "\n",
      "    accuracy                           0.99      8980\n",
      "   macro avg       0.99      0.99      0.99      8980\n",
      "weighted avg       0.99      0.99      0.99      8980\n",
      "\n",
      "Linear SVC:               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      4696\n",
      "           1       1.00      1.00      1.00      4284\n",
      "\n",
      "    accuracy                           1.00      8980\n",
      "   macro avg       1.00      1.00      1.00      8980\n",
      "weighted avg       1.00      1.00      1.00      8980\n",
      "\n",
      "Multinomial NB:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.94      0.95      4696\n",
      "           1       0.93      0.97      0.95      4284\n",
      "\n",
      "    accuracy                           0.95      8980\n",
      "   macro avg       0.95      0.95      0.95      8980\n",
      "weighted avg       0.95      0.95      0.95      8980\n",
      "\n",
      "Random Forest:               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99      4696\n",
      "           1       0.99      1.00      0.99      4284\n",
      "\n",
      "    accuracy                           0.99      8980\n",
      "   macro avg       0.99      0.99      0.99      8980\n",
      "weighted avg       0.99      0.99      0.99      8980\n",
      "\n",
      "Gradient Boosting:               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00      4696\n",
      "           1       0.99      1.00      1.00      4284\n",
      "\n",
      "    accuracy                           1.00      8980\n",
      "   macro avg       1.00      1.00      1.00      8980\n",
      "weighted avg       1.00      1.00      1.00      8980\n",
      "\n",
      "Model Comparison:\n",
      "                 Model  Accuracy\n",
      "1           Linear SVC  0.996325\n",
      "4    Gradient Boosting  0.996214\n",
      "3        Random Forest  0.992873\n",
      "0  Logistic Regression  0.988085\n",
      "2       Multinomial NB  0.952450\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# We have to input 'text' as the only input to the models --> prevent data leakage \n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[\"text\"], df[\"label\"], test_size=0.2, random_state=42, stratify=df[\"label\"]\n",
    ")\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.7)\n",
    "tfidf_train = vectorizer.fit_transform(X_train)\n",
    "tfidf_test = vectorizer.transform(X_test)\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=250),\n",
    "    \"Linear SVC\": LinearSVC(),\n",
    "    \"Multinomial NB\": MultinomialNB(),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=200, random_state=40),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=40)\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(tfidf_train, y_train)\n",
    "    y_pred = model.predict(tfidf_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    results.append({\"Model\": name, \"Accuracy\": acc})\n",
    "    print(f\"{name}:\", classification_report(y_test, y_pred))\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values(by=\"Accuracy\", ascending=False)\n",
    "print(\"Model Comparison:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "811f1afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fake sample words: ['xv', 'scaramuccithen', 'readjoe', 'idiotwatch', 'conyersnancypelosi', 'rlaamong', 'uscharlottesville', 'airwaysother', 'observedhe', 'investigationjudge', 'yannikouts', 'nowthisconservative', 'pictwittercomkqzuwqalok', 'industryflorida', 'properlyis', 'koldobandini', 'facebookwith', 'articleperhaps', 'towntrump', 'pictwittercomdygigiyrs']\n",
      "Real sample words : ['graceland', '“victims', 'chicagowashington', 'spenders”', 'dreamhost’s', 'vista’s', 'kurdishinhabited', 'markit’s', 'baic', 'selth', '“respecting', 'toernes', 'rauners’', 'alghani', 'karad', 'republic”', 'nastywomen', 'gbwide', 'fullsenate', 'questions”']\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "fake_words = Counter(\" \".join(df[df[\"label\"]==0][\"text\"]).split())\n",
    "real_words = Counter(\" \".join(df[df[\"label\"]==1][\"text\"]).split())\n",
    "\n",
    "exclusive_fake = set(fake_words) - set(real_words)\n",
    "exclusive_real = set(real_words) - set(fake_words)\n",
    "\n",
    "print(\"Fake sample words:\", list(exclusive_fake)[:20])\n",
    "print(\"Real sample words :\", list(exclusive_real)[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b6e297e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dup news : 11426\n"
     ]
    }
   ],
   "source": [
    "duplicates = df.duplicated(subset=[\"text\"], keep=False)\n",
    "print(\" dup news :\", duplicates.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9780cbdc",
   "metadata": {},
   "source": [
    "### phase 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "840fcf85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": 38583\n"
     ]
    }
   ],
   "source": [
    "#delete duplicated\n",
    "df_clean = df.drop_duplicates(subset=[\"text\"]).reset_index(drop=True)\n",
    "print(f\": {len(df_clean)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8db058ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing agian\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)               \n",
    "    text = re.sub(r\"\\d+\", \"\", text)                  \n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation)) \n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "df_clean[\"clean_text\"] = df_clean[\"text\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cd5419b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify exclusive words\n",
    "\n",
    "fake_words = Counter(\" \".join(df_clean[df_clean[\"label\"]==0][\"clean_text\"]).split())\n",
    "real_words = Counter(\" \".join(df_clean[df_clean[\"label\"]==1][\"clean_text\"]).split())\n",
    "\n",
    "exclusive_fake = set(fake_words) - set(real_words)\n",
    "exclusive_real = set(real_words) - set(fake_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9d4e451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete exclusive\n",
    "\n",
    "def remove_exclusive_and_rare_words(text, rare_threshold=2):\n",
    "    words = text.split()\n",
    "    words = [w for w in words if w not in exclusive_fake and w not in exclusive_real]\n",
    "    counts = Counter(words)\n",
    "    words = [w for w in words if counts[w] >= rare_threshold]\n",
    "    return \" \".join(words)\n",
    "\n",
    "df_clean[\"clean_text\"] = df_clean[\"clean_text\"].apply(remove_exclusive_and_rare_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb758a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test and vectorize\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_clean[\"clean_text\"], df_clean[\"label\"], test_size=0.2,\n",
    "    random_state=42, stratify=df_clean[\"label\"]\n",
    ")\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.7, min_df=3, ngram_range=(1, 2))\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "899eae79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after cleaning: 0.9425942723856421\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.93      0.94      3479\n",
      "           1       0.95      0.95      0.95      4238\n",
      "\n",
      "    accuracy                           0.94      7717\n",
      "   macro avg       0.94      0.94      0.94      7717\n",
      "weighted avg       0.94      0.94      0.94      7717\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(max_iter=200)\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "print(\"Accuracy after cleaning:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a47bfc9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVC Accuracy: 0.9492\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.95      0.94      3479\n",
      "           1       0.96      0.95      0.95      4238\n",
      "\n",
      "    accuracy                           0.95      7717\n",
      "   macro avg       0.95      0.95      0.95      7717\n",
      "weighted avg       0.95      0.95      0.95      7717\n",
      "\n",
      "Gradient Boosting Accuracy: 0.8857\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.90      0.88      3479\n",
      "           1       0.91      0.87      0.89      4238\n",
      "\n",
      "    accuracy                           0.89      7717\n",
      "   macro avg       0.88      0.89      0.89      7717\n",
      "weighted avg       0.89      0.89      0.89      7717\n",
      "\n",
      "Random Forest Accuracy: 0.9388\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.92      0.93      3479\n",
      "           1       0.93      0.96      0.94      4238\n",
      "\n",
      "    accuracy                           0.94      7717\n",
      "   macro avg       0.94      0.94      0.94      7717\n",
      "weighted avg       0.94      0.94      0.94      7717\n",
      "\n",
      "Multinomial NB Accuracy: 0.9054\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.85      0.89      3479\n",
      "           1       0.89      0.95      0.92      4238\n",
      "\n",
      "    accuracy                           0.91      7717\n",
      "   macro avg       0.91      0.90      0.90      7717\n",
      "weighted avg       0.91      0.91      0.90      7717\n",
      "\n",
      "Logistic Regression Accuracy: 0.9426\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.93      0.94      3479\n",
      "           1       0.95      0.95      0.95      4238\n",
      "\n",
      "    accuracy                           0.94      7717\n",
      "   macro avg       0.94      0.94      0.94      7717\n",
      "weighted avg       0.94      0.94      0.94      7717\n",
      "\n",
      "Summary of accuracies:\n",
      "Linear SVC: 0.9492\n",
      "Gradient Boosting: 0.8857\n",
      "Random Forest: 0.9388\n",
      "Multinomial NB: 0.9054\n",
      "Logistic Regression: 0.9426\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    \"Linear SVC\": LinearSVC(max_iter=3000),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=250, random_state=40, n_jobs=-1),\n",
    "    \"Multinomial NB\": MultinomialNB(),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=250)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_tfidf, y_train)\n",
    "    preds = model.predict(X_test_tfidf)\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    results[name] = acc\n",
    "    print(f\"{name} Accuracy: {acc:.4f}\")\n",
    "    print(classification_report(y_test, preds))\n",
    "\n",
    "print(\"Summary of accuracies:\")\n",
    "for k, v in results.items():\n",
    "    print(f\"{k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d19bcd",
   "metadata": {},
   "source": [
    "### Hyper-Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2b7e8f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'C': np.logspace(-3, 2, 6),  # [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "    'loss': ['hinge', 'squared_hinge'],\n",
    "    'class_weight': [None, 'balanced']\n",
    "}\n",
    "\n",
    "# base model\n",
    "svc = LinearSVC(max_iter=5000)\n",
    "\n",
    "# set GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=svc,\n",
    "    param_grid=param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=5,              # 5-fold cross validation\n",
    "    n_jobs=-1,   \n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e030cc97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Best Parameters: {'C': np.float64(1.0), 'class_weight': None, 'loss': 'hinge'}\n",
      "Best Cross-Validation Accuracy: 0.9502040165614586\n",
      "Test Accuracy with best params: 0.9484255539717507\n"
     ]
    }
   ],
   "source": [
    "#search for the best option\n",
    "grid_search.fit(X_train_tfidf, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Cross-Validation Accuracy:\", grid_search.best_score_)\n",
    "\n",
    "#test on the test data\n",
    "best_model = grid_search.best_estimator_\n",
    "test_acc = best_model.score(X_test_tfidf, y_test)\n",
    "print(\"Test Accuracy with best params:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49ccba4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
